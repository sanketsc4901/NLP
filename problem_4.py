# -*- coding: utf-8 -*-
"""problem 4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WlPXDNVuxdzymhRCpbZQlK4x08US8zRD
"""

import numpy as np
import matplotlib.pyplot as plt

# Assuming you have already defined distinct_words and co_occurrence_matrix functions

corpus = ["Virat", "Kohli", "is", "the", "greatest", "batsman", "of", "this", "generation"]

# Get the co-occurrence matrix
co_matrix, words = co_occurrence_matrix(corpus)

# Perform dimensionality reduction using PCA
embeddings = dimensionality_reduction(co_matrix, k=2)

# Print the reduced embeddings
print("Reduced Embeddings:")
print(embeddings)

# Visualize the 2D embeddings
plt.figure(figsize=(10, 8))
for i, word in enumerate(words):
    plt.scatter(embeddings[i, 0], embeddings[i, 1], marker='o', color='blue')
    plt.text(embeddings[i, 0] + 0.01, embeddings[i, 1] + 0.01, word, fontsize=9)

plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.title('2D Word Embeddings after Dimensionality Reduction')
plt.show()