# -*- coding: utf-8 -*-
"""problem 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kp4Xqcfk_Yif9LWUbvgsl_KVPHakUqyX
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Function to work out the distinct words (word types) that occur in the corpus.
def distinct_words(corpus):
    return list(set(corpus))

# Function to construct a co-occurrence matrix for a certain window-size n
def co_occurrence_matrix(corpus, window_size=4):
    words = distinct_words(corpus)
    word_to_index = {word: i for i, word in enumerate(words)}
    num_words = len(words)
    co_matrix = np.zeros((num_words, num_words), dtype=int)

    for i, target_word in enumerate(corpus):
        target_index = word_to_index[target_word]

        start = max(0, i - window_size)
        end = min(len(corpus), i + window_size + 1)

        context_words = [corpus[j] for j in range(start, end) if j != i]
        for context_word in context_words:
            context_index = word_to_index[context_word]
            co_matrix[target_index, context_index] += 1

    return co_matrix, words

# Function to perform dimensionality reduction on the matrix to produce k-dimensional embeddings
def dimensionality_reduction(co_matrix, k=2):
    pca = PCA(n_components=k)
    embeddings = pca.fit_transform(co_matrix)
    return embeddings

# Function to plot a set of 2D vectors in 2D space
def plot_embeddings(embeddings, words):
    plt.figure(figsize=(10, 8))
    for i, word in enumerate(words):
        plt.scatter(embeddings[i, 0], embeddings[i, 1], marker='o', color='blue')
        plt.text(embeddings[i, 0] + 0.01, embeddings[i, 1] + 0.01, word, fontsize=9)

    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.title('2D Word Embeddings')
    plt.show(block=True)

# Load data and create embeddings
corpus = ["Virat", "Kohli", "is", "the", "greatest", "batsman", "of", "this", "generation"]
co_matrix, words = co_occurrence_matrix(corpus)
embeddings = dimensionality_reduction(co_matrix, k=2)

# Call the plot function
plot_embeddings(embeddings, words)